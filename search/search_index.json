{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to CAST AI documentation \u00b6 CAST AI is an intelligent engine that uses your cloud service accounts to create the cloud resources you need and sets up a multi-cloud cluster for you. You can start using multi-cloud Kubernetes with just a few clicks. The documentation is divided into the following sections: Getting started guides : if you're new to CAST AI, read this first. Administration : cluster administrator's reference. Developer guides : configuring deployments, volumes, scaling policies, etc.","title":"Introduction"},{"location":"#welcome-to-cast-ai-documentation","text":"CAST AI is an intelligent engine that uses your cloud service accounts to create the cloud resources you need and sets up a multi-cloud cluster for you. You can start using multi-cloud Kubernetes with just a few clicks. The documentation is divided into the following sections: Getting started guides : if you're new to CAST AI, read this first. Administration : cluster administrator's reference. Developer guides : configuring deployments, volumes, scaling policies, etc.","title":"Welcome to CAST AI documentation"},{"location":"getting-help/","text":"Getting help \u00b6 Getting stuck sucks, thats why CAST AI has a few different ways of getting help: Slack community channel : Our customer support, developers and community members are willing to find the answers! https://join.slack.com/t/castai-community/shared_invite/zt-i8fcn2xi-sM_iONKn35NmYR2E3dtfng Live chat or email: Our customer support team is reachable via email: support@cast.ai or live chat on our website. Security Bugs \u00b6 We strongly encourage you to report security vulnerabilities or any other bug to our email: Bugbounty@cast.ai - first, before disclosing them publicly.","title":"Getting help"},{"location":"getting-help/#getting-help","text":"Getting stuck sucks, thats why CAST AI has a few different ways of getting help: Slack community channel : Our customer support, developers and community members are willing to find the answers! https://join.slack.com/t/castai-community/shared_invite/zt-i8fcn2xi-sM_iONKn35NmYR2E3dtfng Live chat or email: Our customer support team is reachable via email: support@cast.ai or live chat on our website.","title":"Getting help"},{"location":"getting-help/#security-bugs","text":"We strongly encourage you to report security vulnerabilities or any other bug to our email: Bugbounty@cast.ai - first, before disclosing them publicly.","title":"Security Bugs"},{"location":"api/authentication/","text":"Authentication \u00b6 Before you can use our API, either with your preferred REST client, or via Terraform, you'll need an API key. Obtaining API access key \u00b6 From the top menu, open API | API access keys . Clicking on \"create access key\" opens a dialog where all you need is to name your key. Use descriptive name for your intended purpose - later, when you have more keys, it will be easier to distinguish which key is used for which integration. After key is created, you are presented with a key itself. Save that key, as you won't be able to retrieve it again after this window is closed. The reason API key value is visible at create time only is because we don't store the key in plain text on our system. For security reasons, CAST AI \"forgets\" key value after giving it to you, and later is only able to verify if key is valid, but not to re-retrieve the value for you. If you lose your key, the only solution is to create a new key. Setting up CAST AI Swagger \u00b6 You can test your key directly in our API specification . Visit https://api.cast.ai/v1/spec/ , click \"Authorize\" and enter your key for X-API-Key field. After setting this up, you are now ready to use \"Try it out\" button that is available for each endpoint. Using keys in API calls \u00b6 To authenticate, provide the key in X-API-Key HTTP header. For example, for curl this would be: curl -X GET \"https://api.cast.ai/v1/kubernetes/clusters\" -H \"X-API-Key: your-api-key-here\" | jq","title":"Authentication"},{"location":"api/authentication/#authentication","text":"Before you can use our API, either with your preferred REST client, or via Terraform, you'll need an API key.","title":"Authentication"},{"location":"api/authentication/#obtaining-api-access-key","text":"From the top menu, open API | API access keys . Clicking on \"create access key\" opens a dialog where all you need is to name your key. Use descriptive name for your intended purpose - later, when you have more keys, it will be easier to distinguish which key is used for which integration. After key is created, you are presented with a key itself. Save that key, as you won't be able to retrieve it again after this window is closed. The reason API key value is visible at create time only is because we don't store the key in plain text on our system. For security reasons, CAST AI \"forgets\" key value after giving it to you, and later is only able to verify if key is valid, but not to re-retrieve the value for you. If you lose your key, the only solution is to create a new key.","title":"Obtaining API access key"},{"location":"api/authentication/#setting-up-cast-ai-swagger","text":"You can test your key directly in our API specification . Visit https://api.cast.ai/v1/spec/ , click \"Authorize\" and enter your key for X-API-Key field. After setting this up, you are now ready to use \"Try it out\" button that is available for each endpoint.","title":"Setting up CAST AI Swagger"},{"location":"api/authentication/#using-keys-in-api-calls","text":"To authenticate, provide the key in X-API-Key HTTP header. For example, for curl this would be: curl -X GET \"https://api.cast.ai/v1/kubernetes/clusters\" -H \"X-API-Key: your-api-key-here\" | jq","title":"Using keys in API calls"},{"location":"api/overview/","text":"Accessing CAST AI services via API \u00b6 Overview \u00b6 We build our services at CAST AI API-first; anything you can do in our console UI is available via REST API. You can use either your prefered way to call REST services directly, or leverage our Terraform plugin automate your infrastructure provisioning.","title":"Introduction"},{"location":"api/overview/#accessing-cast-ai-services-via-api","text":"","title":"Accessing CAST AI services via API"},{"location":"api/overview/#overview","text":"We build our services at CAST AI API-first; anything you can do in our console UI is available via REST API. You can use either your prefered way to call REST services directly, or leverage our Terraform plugin automate your infrastructure provisioning.","title":"Overview"},{"location":"api/specification/","text":"API specification \u00b6 Our API contract is published as OpenAPI v3 specification. The simplest way to check it out is to visit our Swagger UI, which is available at: https://api.cast.ai This will bring your to our current specification, where you'll be able to familiarize yourself with available APIs, as well as try functionality directly in the browser. Note Trying out APIs in the browser requires setting up the UI with an API Key. See Authentication for more details. We don't maintain any public SDKs, but it is sufficiently trivial to generate an API client for your programming language using many of the OpenAPI generators . Use below json as a spec: https://api.cast.ai/v1/spec/openapi.json Note OpenAPI is widely supported. Many tools, e.g. Postman, allow importing OpenAPI definitions as well. See documentation for your REST tooling to find out more.","title":"API specification"},{"location":"api/specification/#api-specification","text":"Our API contract is published as OpenAPI v3 specification. The simplest way to check it out is to visit our Swagger UI, which is available at: https://api.cast.ai This will bring your to our current specification, where you'll be able to familiarize yourself with available APIs, as well as try functionality directly in the browser. Note Trying out APIs in the browser requires setting up the UI with an API Key. See Authentication for more details. We don't maintain any public SDKs, but it is sufficiently trivial to generate an API client for your programming language using many of the OpenAPI generators . Use below json as a spec: https://api.cast.ai/v1/spec/openapi.json Note OpenAPI is widely supported. Many tools, e.g. Postman, allow importing OpenAPI definitions as well. See documentation for your REST tooling to find out more.","title":"API specification"},{"location":"api/terraform-provider/","text":"Terraform provider \u00b6 Your CAST.AI infrastructure can be automated via Terraform using terraform-provider-castai provider. Installation steps, example projects and releases are available at the repository: https://github.com/castai/terraform-provider-castai .","title":"Terraform provider"},{"location":"api/terraform-provider/#terraform-provider","text":"Your CAST.AI infrastructure can be automated via Terraform using terraform-provider-castai provider. Installation steps, example projects and releases are available at the repository: https://github.com/castai/terraform-provider-castai .","title":"Terraform provider"},{"location":"concepts/architecture-overview/","text":"Cluster architecture overview \u00b6 This chapter summarizes the overall design of your Kubernetes cluster and how it's relationships with CAST AI platform. Cluster lifecycle \u00b6 Provisioning \u00b6 You initiate creation of the cluster by providing CAST AI with: Access to your Cloud accounts - CAST AI uses these to call cloud APIs to build your infrastructure for you; Initial configuration of your cluster, like region or size of the control plane. We aim to keep these options to a minimum and use our own opinionated setup where appropriate. Reconciliation & healing \u00b6 All clusters created on CAST AI enter a reconciliation loop, where platform periodically re-checks that actual infrastructure on your cloud reflects the specified configuration, and performs upgrades & patching. Reconciliation performs checks such as: Is cluster network configuration up to date; Are any nodes missing, e.g. accidentally deleted; Are there any dangling resources on your cloud associated with your cluster to clean up. Resizing \u00b6 You'll notice that CAST AI clusters don't have a \"node pool\" concept you might be familiar with. Instead, you can choose specific node configuration to be added whenever you need to expand the cluster, or select specific nodes to delete when shrinking it. Same applies to autoscaling engine - it performs decisions per-node level, instead of choosing to grow or shrink a node pool. Cleanup \u00b6 When you instruct CAST to delete your cluster, in general case platform will just try to collapse created cloud resources in the fastest way. Keep in mind that nodes will not be drained before deleting them, and any running workloads won't be given a chance to terminate gracefully. Deletion aims to minimize unintended removals. For example, virtual machines on AWS are deleted by a specific tag containing cluster UUID. If any additional VMs remain present in cluster's security group, that security group won't be deleted and you'll see delete operation fail. Cluster architecture \u00b6 Context \u00b6 Below diagram highlights primary groups of components that define a relationship between CAST AI platform and your cluster. New CAST AI users will start by interacting with the platform via console UI ( https://console.cast.ai ). Once the created cluster is ready, by downloading cluster's kubeconfig you are able to access your cluster directly. Some of the middleware that is running on the cluster (Grafana, Kubernetes dashboard) is directly reachable from UI through the single-signon gateway. You can notice that there's a bi-direction link between your cluster and CAST AI platform. Not only the platform connects to your cloud infrastructure or the cluster itself; CAST AI also relies on the cluster to \"call back\" and inform about certain events: Cluster control plane nodes actions with provisioning engine, e.g. when to join the cluster; Nodes inform about operations being completed, like finishing joining the cluster; Relevant cloud events get propagated to provisioning engine & autoscaler, for example, \"spot instance is being terminated by cloud provider\"; Your app users don't interact with CAST AI in any way. You own your kubernetes cluster infrastructure 100%, including any ingress infrastructure to reach your cluster workloads. Cluster infrastructure \u00b6 Nodes \u00b6 Here's the overview on where cluster virtual machines will be provisioned on your cloud. Ingress \u00b6 CAST AI provisioned clusters contain all the infrastructure needed to equip your app with an external TLS endpoint: DNS entry to round-robin; Load-balancing infrastructure: cloud-native load balancers that route traffic to sub-section of your cluster (e.g. traffic that hits AWS load balancer will route to AWS nodes); Nginx ingress controller, paired with TLS certificate manager, that listen to your deployed resources and maintain routing&TLS configuration; Metric collection for your ingress traffic; All that is left for you as an application developer is to deploy your app, ingress resource, and configure a domain alias of your choice. See the guide for more details. Network details \u00b6 Region & zone \u00b6 As you select a Cast region, for each cloud it maps to a specific region on that cloud. For example, US East (Ashburn) region maps to: AWS: us-east-1 GCP: us-east4 Azure: eastus Digital Ocean: nyc1 Currently, on each cloud CAST AI builds a single-zone setup of your cluster. Zone selection is cloud-specific. Master nodes inbound \u00b6 Protocol Port Source Description tcp 6443 0.0.0.0/0 k8s API server udp 51820 0.0.0.0/0 WireGuard (if used) Worker nodes inbound \u00b6 Protocol Port Source Description udp 51820 0.0.0.0/0 WireGuard (if used) tcp/udp NodePort 0.0.0.0/0 k8s Service with type=LoadBalancer Subnets \u00b6 Range Description 10.96.0.0/12 k8s services 10.217.0.0/16 k8s pods 10.4.0.0/16 WireGuard 10.0.0.0/16 GCP VPC. Smaller /24 blocks are allocated for subnets. 10.10.0.0/16 AWS VPC. Smaller /24 blocks are allocated for subnets. 10.20.0.0/16 AZURE VPC. Smaller /24 blocks are allocated for subnets. 10.100-255.0.0/20 DigitalOcean VPC. There is only one subnet which is allocated dynamically.","title":"Cluster architecture overview"},{"location":"concepts/architecture-overview/#cluster-architecture-overview","text":"This chapter summarizes the overall design of your Kubernetes cluster and how it's relationships with CAST AI platform.","title":"Cluster architecture overview"},{"location":"concepts/architecture-overview/#cluster-lifecycle","text":"","title":"Cluster lifecycle"},{"location":"concepts/architecture-overview/#provisioning","text":"You initiate creation of the cluster by providing CAST AI with: Access to your Cloud accounts - CAST AI uses these to call cloud APIs to build your infrastructure for you; Initial configuration of your cluster, like region or size of the control plane. We aim to keep these options to a minimum and use our own opinionated setup where appropriate.","title":"Provisioning"},{"location":"concepts/architecture-overview/#reconciliation-healing","text":"All clusters created on CAST AI enter a reconciliation loop, where platform periodically re-checks that actual infrastructure on your cloud reflects the specified configuration, and performs upgrades & patching. Reconciliation performs checks such as: Is cluster network configuration up to date; Are any nodes missing, e.g. accidentally deleted; Are there any dangling resources on your cloud associated with your cluster to clean up.","title":"Reconciliation &amp; healing"},{"location":"concepts/architecture-overview/#resizing","text":"You'll notice that CAST AI clusters don't have a \"node pool\" concept you might be familiar with. Instead, you can choose specific node configuration to be added whenever you need to expand the cluster, or select specific nodes to delete when shrinking it. Same applies to autoscaling engine - it performs decisions per-node level, instead of choosing to grow or shrink a node pool.","title":"Resizing"},{"location":"concepts/architecture-overview/#cleanup","text":"When you instruct CAST to delete your cluster, in general case platform will just try to collapse created cloud resources in the fastest way. Keep in mind that nodes will not be drained before deleting them, and any running workloads won't be given a chance to terminate gracefully. Deletion aims to minimize unintended removals. For example, virtual machines on AWS are deleted by a specific tag containing cluster UUID. If any additional VMs remain present in cluster's security group, that security group won't be deleted and you'll see delete operation fail.","title":"Cleanup"},{"location":"concepts/architecture-overview/#cluster-architecture","text":"","title":"Cluster architecture"},{"location":"concepts/architecture-overview/#context","text":"Below diagram highlights primary groups of components that define a relationship between CAST AI platform and your cluster. New CAST AI users will start by interacting with the platform via console UI ( https://console.cast.ai ). Once the created cluster is ready, by downloading cluster's kubeconfig you are able to access your cluster directly. Some of the middleware that is running on the cluster (Grafana, Kubernetes dashboard) is directly reachable from UI through the single-signon gateway. You can notice that there's a bi-direction link between your cluster and CAST AI platform. Not only the platform connects to your cloud infrastructure or the cluster itself; CAST AI also relies on the cluster to \"call back\" and inform about certain events: Cluster control plane nodes actions with provisioning engine, e.g. when to join the cluster; Nodes inform about operations being completed, like finishing joining the cluster; Relevant cloud events get propagated to provisioning engine & autoscaler, for example, \"spot instance is being terminated by cloud provider\"; Your app users don't interact with CAST AI in any way. You own your kubernetes cluster infrastructure 100%, including any ingress infrastructure to reach your cluster workloads.","title":"Context"},{"location":"concepts/architecture-overview/#cluster-infrastructure","text":"","title":"Cluster infrastructure"},{"location":"concepts/architecture-overview/#nodes","text":"Here's the overview on where cluster virtual machines will be provisioned on your cloud.","title":"Nodes"},{"location":"concepts/architecture-overview/#ingress","text":"CAST AI provisioned clusters contain all the infrastructure needed to equip your app with an external TLS endpoint: DNS entry to round-robin; Load-balancing infrastructure: cloud-native load balancers that route traffic to sub-section of your cluster (e.g. traffic that hits AWS load balancer will route to AWS nodes); Nginx ingress controller, paired with TLS certificate manager, that listen to your deployed resources and maintain routing&TLS configuration; Metric collection for your ingress traffic; All that is left for you as an application developer is to deploy your app, ingress resource, and configure a domain alias of your choice. See the guide for more details.","title":"Ingress"},{"location":"concepts/architecture-overview/#network-details","text":"","title":"Network details"},{"location":"concepts/architecture-overview/#region-zone","text":"As you select a Cast region, for each cloud it maps to a specific region on that cloud. For example, US East (Ashburn) region maps to: AWS: us-east-1 GCP: us-east4 Azure: eastus Digital Ocean: nyc1 Currently, on each cloud CAST AI builds a single-zone setup of your cluster. Zone selection is cloud-specific.","title":"Region &amp; zone"},{"location":"concepts/architecture-overview/#master-nodes-inbound","text":"Protocol Port Source Description tcp 6443 0.0.0.0/0 k8s API server udp 51820 0.0.0.0/0 WireGuard (if used)","title":"Master nodes inbound"},{"location":"concepts/architecture-overview/#worker-nodes-inbound","text":"Protocol Port Source Description udp 51820 0.0.0.0/0 WireGuard (if used) tcp/udp NodePort 0.0.0.0/0 k8s Service with type=LoadBalancer","title":"Worker nodes inbound"},{"location":"concepts/architecture-overview/#subnets","text":"Range Description 10.96.0.0/12 k8s services 10.217.0.0/16 k8s pods 10.4.0.0/16 WireGuard 10.0.0.0/16 GCP VPC. Smaller /24 blocks are allocated for subnets. 10.10.0.0/16 AWS VPC. Smaller /24 blocks are allocated for subnets. 10.20.0.0/16 AZURE VPC. Smaller /24 blocks are allocated for subnets. 10.100-255.0.0/20 DigitalOcean VPC. There is only one subnet which is allocated dynamically.","title":"Subnets"},{"location":"concepts/how-it-works/","text":"How it works \u00b6 Long story short, the CAST AI engine uses your Cloud Service Provider (CSP) accounts to create the required cloud resources and set up a multi-cloud cluster for you. You can start using multi-cloud Kubernetes with just a few clicks. To get a bit more technical; using your owned and provided CSP accounts, CAST AI creates VPCs or Resource Groups (depending on which cloud services you use). Next, CAST AI creates the required network (like subnets, public IPs, and VPNs). This will be used to ensure a uniform network across created VPCs, which is required for a seamless Kubernetes operation. There are certain processes in place to help non-compatible clouds merge into a single flat network. CAST AI regions are selected with network latency in mind. For your applications and the cluster to function as expected, cross-cloud latency shouldn't go above 10 ms in normal operation. The CAST AI regions were measured to operate in a 5-7 ms range. Once we have the network in place, VMs are added to take the role of Kubernetes Masters and Workers. You can add or remove Worker nodes later on, based on your needs. But currently, the count and size of Master nodes are set during cluster creation. Later on, the same CAST AI engine reconciles the created cluster every hour to make sure that it\u2019s still in the desired state. In this context, reconciling means going through all your cloud resources and ensuring the required configuration. If you delete any resources from the provided CSP accounts manually, CAST AI recreates them to the specification provided by you in the console. During the time of reconciliation, no instant changes to the cluster are allowed. You can only apply them after the reconciliation. When you\u2019re done with your cluster, you can delete it via the CAST AI console. This operation will terminate all VMs and delete cloud resources like the attached storage, public IPs, VPN connections, network subnets, etc. Basically, this makes your cloud accounts look like prior to using CAST AI Kubernetes.","title":"How it works"},{"location":"concepts/how-it-works/#how-it-works","text":"Long story short, the CAST AI engine uses your Cloud Service Provider (CSP) accounts to create the required cloud resources and set up a multi-cloud cluster for you. You can start using multi-cloud Kubernetes with just a few clicks. To get a bit more technical; using your owned and provided CSP accounts, CAST AI creates VPCs or Resource Groups (depending on which cloud services you use). Next, CAST AI creates the required network (like subnets, public IPs, and VPNs). This will be used to ensure a uniform network across created VPCs, which is required for a seamless Kubernetes operation. There are certain processes in place to help non-compatible clouds merge into a single flat network. CAST AI regions are selected with network latency in mind. For your applications and the cluster to function as expected, cross-cloud latency shouldn't go above 10 ms in normal operation. The CAST AI regions were measured to operate in a 5-7 ms range. Once we have the network in place, VMs are added to take the role of Kubernetes Masters and Workers. You can add or remove Worker nodes later on, based on your needs. But currently, the count and size of Master nodes are set during cluster creation. Later on, the same CAST AI engine reconciles the created cluster every hour to make sure that it\u2019s still in the desired state. In this context, reconciling means going through all your cloud resources and ensuring the required configuration. If you delete any resources from the provided CSP accounts manually, CAST AI recreates them to the specification provided by you in the console. During the time of reconciliation, no instant changes to the cluster are allowed. You can only apply them after the reconciliation. When you\u2019re done with your cluster, you can delete it via the CAST AI console. This operation will terminate all VMs and delete cloud resources like the attached storage, public IPs, VPN connections, network subnets, etc. Basically, this makes your cloud accounts look like prior to using CAST AI Kubernetes.","title":"How it works"},{"location":"getting-started/configuring-aws-credentials/","text":"Configure AWS credentials \u00b6 After completing the following instructions, you\u2019ll retrieve Access key ID and Secret access key. These credentials are by CAST AI for creating a cluster with AWS resources. Open https://console.aws.amazon.com/ . Go to IAM service. In the Users section, click on Add user: Enter the User name, select Programmatic access type, and click next (permissions): Click Create group, enter the Group name, and select the following permission policies: AmazonVPCFullAccess AmazonEC2FullAccess IAMFullAccess Click the Create group button again. Click next (tags) \u2192 next (review) \u2192 create user. You will end up on a screen where you can retrieve credentials in AWS GUI or download credentials containing .csv file.","title":"Configure AWS credentials"},{"location":"getting-started/configuring-aws-credentials/#configure-aws-credentials","text":"After completing the following instructions, you\u2019ll retrieve Access key ID and Secret access key. These credentials are by CAST AI for creating a cluster with AWS resources. Open https://console.aws.amazon.com/ . Go to IAM service. In the Users section, click on Add user: Enter the User name, select Programmatic access type, and click next (permissions): Click Create group, enter the Group name, and select the following permission policies: AmazonVPCFullAccess AmazonEC2FullAccess IAMFullAccess Click the Create group button again. Click next (tags) \u2192 next (review) \u2192 create user. You will end up on a screen where you can retrieve credentials in AWS GUI or download credentials containing .csv file.","title":"Configure AWS credentials"},{"location":"getting-started/configuring-azure-credentials/","text":"Configure Azure credentials \u00b6 The instructions we provide in this section will guide you in creating an external app registration. After completing the instructions, you\u2019ll get keys similar to the following that allow CAST AI to access your Azure Subscription resources: tenant id: 355069b9-fbb7-yyyy-xxxx-83d5a8709111 client id: 2d970aa5-4dde-yyyy-xxxx-36581dfff222 client secret: xvKds90f83&&#$$qwerty subscription id: 697e39d3-4b01-yyyy-xxxx-75266e90c333 Open https://portal.azure.com/ and go to the App registrations section: Click on New registration and complete the registration by entering the Name and selecting Multi-tenant access. Once you complete the registration, you\u2019ll end up on the App Overview page. Save the Application (client) ID and Directory (tenant) ID from this page. They represent Tenant ID and Client ID keys from the required outputs of these instructions. Click on Certificates & secrets in the Manage tab and click New client secret . Enter meaningful descriptions and select secret expiry time . Click Add . Copy the secret value you have created. It will be used as Client Secret when you enter it into CAST AI console. Give access to the CAST AI application by requesting a sign-in using a browser \u00b6 1 - Make sure to click the link on our console where you have pasted the keys. 2 - After login, you should see the Permissions requested window. Click Accept which will allow adding CAST AI application role. You should see it like this: Assigning the roles \u00b6 Go to Subscriptions Select subscription to which you want to give CAST AI access Take note of the Subscription ID . This is the last item required for your Azure app credentials. We have one more step left here. Create a Role assignment between the previously created App registration and this Subscription. Go to Access control (IAM) . Click on Add \u2192 Add role assignment The following sidebar will appear In the Role dropdown, select Contributor . In the Select field type your Client Secret (created app secret during the first step). Also you need CAST AI Shared Images as a selected member. Then click save (if the role is not visible please check previous step and try again). This is it. After taking all of the steps above, you should have the required keys to create your first azure cluster!","title":"Configure Azure credentials"},{"location":"getting-started/configuring-azure-credentials/#configure-azure-credentials","text":"The instructions we provide in this section will guide you in creating an external app registration. After completing the instructions, you\u2019ll get keys similar to the following that allow CAST AI to access your Azure Subscription resources: tenant id: 355069b9-fbb7-yyyy-xxxx-83d5a8709111 client id: 2d970aa5-4dde-yyyy-xxxx-36581dfff222 client secret: xvKds90f83&&#$$qwerty subscription id: 697e39d3-4b01-yyyy-xxxx-75266e90c333 Open https://portal.azure.com/ and go to the App registrations section: Click on New registration and complete the registration by entering the Name and selecting Multi-tenant access. Once you complete the registration, you\u2019ll end up on the App Overview page. Save the Application (client) ID and Directory (tenant) ID from this page. They represent Tenant ID and Client ID keys from the required outputs of these instructions. Click on Certificates & secrets in the Manage tab and click New client secret . Enter meaningful descriptions and select secret expiry time . Click Add . Copy the secret value you have created. It will be used as Client Secret when you enter it into CAST AI console.","title":"Configure Azure credentials"},{"location":"getting-started/configuring-azure-credentials/#give-access-to-the-cast-ai-application-by-requesting-a-sign-in-using-a-browser","text":"1 - Make sure to click the link on our console where you have pasted the keys. 2 - After login, you should see the Permissions requested window. Click Accept which will allow adding CAST AI application role. You should see it like this:","title":"Give access to the CAST AI application by requesting a sign-in using a browser"},{"location":"getting-started/configuring-azure-credentials/#assigning-the-roles","text":"Go to Subscriptions Select subscription to which you want to give CAST AI access Take note of the Subscription ID . This is the last item required for your Azure app credentials. We have one more step left here. Create a Role assignment between the previously created App registration and this Subscription. Go to Access control (IAM) . Click on Add \u2192 Add role assignment The following sidebar will appear In the Role dropdown, select Contributor . In the Select field type your Client Secret (created app secret during the first step). Also you need CAST AI Shared Images as a selected member. Then click save (if the role is not visible please check previous step and try again). This is it. After taking all of the steps above, you should have the required keys to create your first azure cluster!","title":"Assigning the roles"},{"location":"getting-started/configuring-do-credentials/","text":"Configure Digital Ocean credentials \u00b6 These are the simple instructions to obtain your API access key from Digital Ocean and create your cluster. First of all, visit your Digital Ocean account. Afterwards click on API in Account panel: Click on Generate New Token : Give it a name and make sure both Read and Write options are selected: You will see your newly created access token. Copy it and paste it in CAST.AI Digital Ocean cloud credential pop-up. That is it! Proceed creating your first Digital Ocean cluster!","title":"Configure Digital Ocean credentials"},{"location":"getting-started/configuring-do-credentials/#configure-digital-ocean-credentials","text":"These are the simple instructions to obtain your API access key from Digital Ocean and create your cluster. First of all, visit your Digital Ocean account. Afterwards click on API in Account panel: Click on Generate New Token : Give it a name and make sure both Read and Write options are selected: You will see your newly created access token. Copy it and paste it in CAST.AI Digital Ocean cloud credential pop-up. That is it! Proceed creating your first Digital Ocean cluster!","title":"Configure Digital Ocean credentials"},{"location":"getting-started/configuring-gcp-credentials/","text":"Configure Google Cloud credentials \u00b6 By following these instructions, you\u2019ll retrieve the Service account JSON credentials. These credentials are required by CAST AI for creating a cluster with GCP resources. Project prerequisites \u00b6 Note that the project where your Service Account is created needs to have the following APIs enabled: Compute API Resource Manager API Please follow the GCP guide on how to enable APIs. Create service account \u00b6 Open https://console.cloud.google.com/ Select your project (or create a new one) in the top bar. Go to the Navigation bar, select IAM & Admin , and then Service accounts : Click Create service account : Enter the preferred Service account name and description . Click Create Add the following roles to the created account: roles/compute.admin roles/iam.serviceAccountUser roles/iam.serviceAccountAdmin roles/iam.roleAdministrator roles/iam.serviceAccountKey roles/iam.projectIAMAdmin Click Save . In the last step of the service account creation, click Done without entering any data. Create key \u00b6 The created account will appear in the Service Accounts list. Click on it to access additional options. In the Keys section, click on Add Key \u2192 Create new key . Select the JSON option and click Create . You\u2019ll get a file download prompt. After the JSON file is downloaded, copy its contents to the input field or click on the Read from file button to import the file.","title":"Configure Google Cloud credentials"},{"location":"getting-started/configuring-gcp-credentials/#configure-google-cloud-credentials","text":"By following these instructions, you\u2019ll retrieve the Service account JSON credentials. These credentials are required by CAST AI for creating a cluster with GCP resources.","title":"Configure Google Cloud credentials"},{"location":"getting-started/configuring-gcp-credentials/#project-prerequisites","text":"Note that the project where your Service Account is created needs to have the following APIs enabled: Compute API Resource Manager API Please follow the GCP guide on how to enable APIs.","title":"Project prerequisites"},{"location":"getting-started/configuring-gcp-credentials/#create-service-account","text":"Open https://console.cloud.google.com/ Select your project (or create a new one) in the top bar. Go to the Navigation bar, select IAM & Admin , and then Service accounts : Click Create service account : Enter the preferred Service account name and description . Click Create Add the following roles to the created account: roles/compute.admin roles/iam.serviceAccountUser roles/iam.serviceAccountAdmin roles/iam.roleAdministrator roles/iam.serviceAccountKey roles/iam.projectIAMAdmin Click Save . In the last step of the service account creation, click Done without entering any data.","title":"Create service account"},{"location":"getting-started/configuring-gcp-credentials/#create-key","text":"The created account will appear in the Service Accounts list. Click on it to access additional options. In the Keys section, click on Add Key \u2192 Create new key . Select the JSON option and click Create . You\u2019ll get a file download prompt. After the JSON file is downloaded, copy its contents to the input field or click on the Read from file button to import the file.","title":"Create key"},{"location":"getting-started/creating-your-first-cluster/","text":"Creating your first cluster \u00b6 This guide covers very basics of creating your first Kubernetes cluster on Cast AI. Setting up cloud credentials \u00b6 Before you're able to create your first cluster, you'll need to provide Cast AI access to your cloud accounts. Our platform will perform actions on your cloud account, like setting up network, security groups, creating VMs and Kubernetes cluster itself. You'll need at least one set of credentials per each cloud you want to use in your multicloud cluster. View, add and delete credentials in console on Cloud Credentials list; you can also add new credentials directly while creating a new cluster . Creating cluster \u00b6 Start by logging into your account. You can create a new cluster by clicking on Create a cluster button in Cast AI console: At this point, we will ask you to enter your cluster name and select its region of multi-cloud providers. Make sure that the cluster name starts with a letter and use hyphens between the letters (no numbers or other characters are allowed). Now you can select your preferred cluster configuration. Note that this is just an initial configuration and it will be adjusted by the scaling and cost optimization policies. The last step is to select a cloud provider and give CAST AI permission to manage your cluster automatically. Click on one of these links to get your key adding instructions for: GCP AWS DIGITAL OCEAN AZURE After finishing this step, simply click on this button and wait a few minutes for the cluster to initialize. Inspecting created cluster \u00b6 After the cluster is started, you will get access to your own metrics and information like: Kubernetes UI Kibana Logs Grafana metrics You can also inspect the cluster nodes in Cast AI console: When checking Grafana, make sure to click on top left Menu to open up your Kubernetes cluster and NGINX ingress controller metrics. Note if you cannot access these metrics right after the cluster is in ready state, give it a minute or two and try again.","title":"Creating your first cluster"},{"location":"getting-started/creating-your-first-cluster/#creating-your-first-cluster","text":"This guide covers very basics of creating your first Kubernetes cluster on Cast AI.","title":"Creating your first cluster"},{"location":"getting-started/creating-your-first-cluster/#setting-up-cloud-credentials","text":"Before you're able to create your first cluster, you'll need to provide Cast AI access to your cloud accounts. Our platform will perform actions on your cloud account, like setting up network, security groups, creating VMs and Kubernetes cluster itself. You'll need at least one set of credentials per each cloud you want to use in your multicloud cluster. View, add and delete credentials in console on Cloud Credentials list; you can also add new credentials directly while creating a new cluster .","title":"Setting up cloud credentials"},{"location":"getting-started/creating-your-first-cluster/#creating-cluster","text":"Start by logging into your account. You can create a new cluster by clicking on Create a cluster button in Cast AI console: At this point, we will ask you to enter your cluster name and select its region of multi-cloud providers. Make sure that the cluster name starts with a letter and use hyphens between the letters (no numbers or other characters are allowed). Now you can select your preferred cluster configuration. Note that this is just an initial configuration and it will be adjusted by the scaling and cost optimization policies. The last step is to select a cloud provider and give CAST AI permission to manage your cluster automatically. Click on one of these links to get your key adding instructions for: GCP AWS DIGITAL OCEAN AZURE After finishing this step, simply click on this button and wait a few minutes for the cluster to initialize.","title":"Creating cluster"},{"location":"getting-started/creating-your-first-cluster/#inspecting-created-cluster","text":"After the cluster is started, you will get access to your own metrics and information like: Kubernetes UI Kibana Logs Grafana metrics You can also inspect the cluster nodes in Cast AI console: When checking Grafana, make sure to click on top left Menu to open up your Kubernetes cluster and NGINX ingress controller metrics. Note if you cannot access these metrics right after the cluster is in ready state, give it a minute or two and try again.","title":"Inspecting created cluster"},{"location":"getting-started/deploying-applications/","text":"Deploying Applications \u00b6 A CAST AI managed Kubernetes cluster is no different than vanilla Kubernetes - it just runs on multi-cloud. Having this in mind, you can deploy an application just like you would in any Kubernetes cluster - by using kubectl . After creating your cluster in the CAST AI console and making sure that it\u2019s in the ready state, you can download the cluster\u2019s kubeconfig : For detailed instructions on how to use a downloaded kubeconfig file, check the official Kubernetes documentation . The quickest way is simply by setting a downloaded file as KUBECONFIG env variable in your shell. For example: export KUBECONFIG=~/Downloads/my-cluster_config Once you do this, you will have access to my-cluster in your used shell. From now on, you can deploy any application using kubectl. Here\u2019s a tutorial from Kubernetes official documentation . If you\u2019re tired of reading documentation and just want to get started, run the following: kubectl apply -f https://k8s.io/examples/application/deployment.yaml The command will use the following deployment YAML file from k8s, for example: apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 2 # tells deployment to run 2 pods matching the template template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 For displaying information about the Deployment run: kubectl describe deployment nginx-deployment The output is similar to this: List the pods created by the deployment: kubectl get pods -l app=nginx The output is similar to this: Display information about a Pod: kubectl describe pod <pod-name> where <pod-name> is the name of one of your Pods.","title":"Deploying Applications"},{"location":"getting-started/deploying-applications/#deploying-applications","text":"A CAST AI managed Kubernetes cluster is no different than vanilla Kubernetes - it just runs on multi-cloud. Having this in mind, you can deploy an application just like you would in any Kubernetes cluster - by using kubectl . After creating your cluster in the CAST AI console and making sure that it\u2019s in the ready state, you can download the cluster\u2019s kubeconfig : For detailed instructions on how to use a downloaded kubeconfig file, check the official Kubernetes documentation . The quickest way is simply by setting a downloaded file as KUBECONFIG env variable in your shell. For example: export KUBECONFIG=~/Downloads/my-cluster_config Once you do this, you will have access to my-cluster in your used shell. From now on, you can deploy any application using kubectl. Here\u2019s a tutorial from Kubernetes official documentation . If you\u2019re tired of reading documentation and just want to get started, run the following: kubectl apply -f https://k8s.io/examples/application/deployment.yaml The command will use the following deployment YAML file from k8s, for example: apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 2 # tells deployment to run 2 pods matching the template template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 For displaying information about the Deployment run: kubectl describe deployment nginx-deployment The output is similar to this: List the pods created by the deployment: kubectl get pods -l app=nginx The output is similar to this: Display information about a Pod: kubectl describe pod <pod-name> where <pod-name> is the name of one of your Pods.","title":"Deploying Applications"},{"location":"getting-started/setting-up-account/","text":"Setting up an account \u00b6 It couldn't get easier than this. Simply open this website and choose one of the three login options: Log in with your Google account Log in with your GitHub account Or you can create an account by entering your email and password: That's it! Now log in and start your multi-cloud Kubernetes journey! Create your free account here!","title":"Setting up an account"},{"location":"getting-started/setting-up-account/#setting-up-an-account","text":"It couldn't get easier than this. Simply open this website and choose one of the three login options: Log in with your Google account Log in with your GitHub account Or you can create an account by entering your email and password: That's it! Now log in and start your multi-cloud Kubernetes journey! Create your free account here!","title":"Setting up an account"},{"location":"guides/autoscaling-policies/","text":"Autoscaling policies \u00b6 Autoscaling policies define a set of rules based on which your cluster is monitored and scaled to maintain steady performance at the lowest possible cost. This topic describes the available policy configuration options and provides guidance on how to configure them. Prerequisites \u00b6 To enable the autoscaling policies, you need to create a cluster first. Here's a guide that shows you how to create a cluster: Creating your first cluster . To see the available policy settings, select your cluster and navigate to Policies on CAST AI's console : Cluster CPU limits policy \u00b6 Each CAST AI cluster size can be limited by the total amount of vCPUs available on all the worker nodes used to run workloads. If disabled, the cluster can upscale indefinitely and downscale to 0 worker nodes, depending on the actual resource consumption. Configuring CPU limits policy \u00b6 You can adjust a cluster's CPU limits settings either via the CAST AI console: or via the CAST AI policies API endpoint by setting values for \"clusterLimits\" : { \"cpu\" : { \"maxCores\" : <value> , \"minCores\" : <value> }, \"enabled\" : <value> } The new settings will propagate immediately. Horizontal Pod Autoscaler (HPA) policy \u00b6 See HPA documentation for a detailed overview. Unscheduled pods policy \u00b6 A pod becomes unschedulable when the Kubernetes scheduler can't find a node that can accommodate the pod. For instance, a pod can request more CPU or memory than the resources available on any of the worker nodes. In many such cases, this indicates the need to scale up by adding additional nodes to the cluster. The CAST AI autoscaler is equipped with a mechanism to handle this. After receiving the unschedulable pods event, the CAST AI recommendation engine will select the best price/performance ratio node able to accommodate all of the currently unschedulable pods. CAST AI will then provision it and join with the cluster. This process usually takes a few minutes, depending on the cloud service provider of your choice. Currently, only a single node will be added at a time. If any unschedulable pods still remain, the cycle is repeated until all the pods are scheduled (provided that the reason was insufficient resources). Configuring the unscheduled pods policy \u00b6 You can enable/disable the unschedulable pods policy either on the CAST AI console: : or via the CAST AI policies API endpoint by setting values for \"unschedulablePods\" : { \"enabled\" : <value> , \"evaluationPeriodSeconds\" : <value> } It may take a few minutes for the new settings to propagate. Cluster CPU utilization scale up policy \u00b6 An increased CPU load on worker nodes indicates that the cluster is getting 'hot' - the fleet of nodes might not be sufficient to fulfill the current computing resources needs. In that case, you can increase the computing capacity by adding in additional worker nodes. CAST AI's cluster autoscaler provides a mechanism to handle this with CPU utilization scale up policy . By applying this policy, your cluster is periodically checked for the actual CPU consumption over the worker nodes. When a sustained increased CPU load is detected, the autoscaler automatically adds a new node to redistribute the load more evenly. This process can take a few minutes, depending on the underlying cloud service provider. If an addition is already in progress, the autoscaler will not attempt to add a new node. Configuring the CPU utilization scale up policy \u00b6 The autoscaler's scale up policy is set by adjusting thresholds for the average cluster CPU load in percentages and evaluation period in seconds. The evaluation window describes for how long the average cluster CPU utilization should stay above the threshold for it to be considered as eligible for scale up. You can edit settings for this policy via the CAST AI console : or the CAST AI policies API endpoint by setting values for \"cpuUtilization\" : { \"scaleUpThreshold\" : { \"avgCpuLoadPercentageMoreThan\" : <value> , \"enabled\" : <value> , \"evaluationPeriodSeconds\" : <value> } } It may take a few minutes for the new settings to propagate. Policies precedence rules \u00b6 If multiple policies are enabled and multiple rules are triggered during the same evaluation period, they will be handled in the following order: Cluster CPU limits policy Horizontal Pod Autoscaler (HPA) policy Unscheduled pods policy Cluster CPU utilization scale up policy","title":"Autoscaling policies"},{"location":"guides/autoscaling-policies/#autoscaling-policies","text":"Autoscaling policies define a set of rules based on which your cluster is monitored and scaled to maintain steady performance at the lowest possible cost. This topic describes the available policy configuration options and provides guidance on how to configure them.","title":"Autoscaling policies"},{"location":"guides/autoscaling-policies/#prerequisites","text":"To enable the autoscaling policies, you need to create a cluster first. Here's a guide that shows you how to create a cluster: Creating your first cluster . To see the available policy settings, select your cluster and navigate to Policies on CAST AI's console :","title":"Prerequisites"},{"location":"guides/autoscaling-policies/#cluster-cpu-limits-policy","text":"Each CAST AI cluster size can be limited by the total amount of vCPUs available on all the worker nodes used to run workloads. If disabled, the cluster can upscale indefinitely and downscale to 0 worker nodes, depending on the actual resource consumption.","title":"Cluster CPU limits policy"},{"location":"guides/autoscaling-policies/#configuring-cpu-limits-policy","text":"You can adjust a cluster's CPU limits settings either via the CAST AI console: or via the CAST AI policies API endpoint by setting values for \"clusterLimits\" : { \"cpu\" : { \"maxCores\" : <value> , \"minCores\" : <value> }, \"enabled\" : <value> } The new settings will propagate immediately.","title":"Configuring CPU limits policy"},{"location":"guides/autoscaling-policies/#horizontal-pod-autoscaler-hpa-policy","text":"See HPA documentation for a detailed overview.","title":"Horizontal Pod Autoscaler (HPA) policy"},{"location":"guides/autoscaling-policies/#unscheduled-pods-policy","text":"A pod becomes unschedulable when the Kubernetes scheduler can't find a node that can accommodate the pod. For instance, a pod can request more CPU or memory than the resources available on any of the worker nodes. In many such cases, this indicates the need to scale up by adding additional nodes to the cluster. The CAST AI autoscaler is equipped with a mechanism to handle this. After receiving the unschedulable pods event, the CAST AI recommendation engine will select the best price/performance ratio node able to accommodate all of the currently unschedulable pods. CAST AI will then provision it and join with the cluster. This process usually takes a few minutes, depending on the cloud service provider of your choice. Currently, only a single node will be added at a time. If any unschedulable pods still remain, the cycle is repeated until all the pods are scheduled (provided that the reason was insufficient resources).","title":"Unscheduled pods policy"},{"location":"guides/autoscaling-policies/#configuring-the-unscheduled-pods-policy","text":"You can enable/disable the unschedulable pods policy either on the CAST AI console: : or via the CAST AI policies API endpoint by setting values for \"unschedulablePods\" : { \"enabled\" : <value> , \"evaluationPeriodSeconds\" : <value> } It may take a few minutes for the new settings to propagate.","title":"Configuring the unscheduled pods policy"},{"location":"guides/autoscaling-policies/#cluster-cpu-utilization-scale-up-policy","text":"An increased CPU load on worker nodes indicates that the cluster is getting 'hot' - the fleet of nodes might not be sufficient to fulfill the current computing resources needs. In that case, you can increase the computing capacity by adding in additional worker nodes. CAST AI's cluster autoscaler provides a mechanism to handle this with CPU utilization scale up policy . By applying this policy, your cluster is periodically checked for the actual CPU consumption over the worker nodes. When a sustained increased CPU load is detected, the autoscaler automatically adds a new node to redistribute the load more evenly. This process can take a few minutes, depending on the underlying cloud service provider. If an addition is already in progress, the autoscaler will not attempt to add a new node.","title":"Cluster CPU utilization scale up policy"},{"location":"guides/autoscaling-policies/#configuring-the-cpu-utilization-scale-up-policy","text":"The autoscaler's scale up policy is set by adjusting thresholds for the average cluster CPU load in percentages and evaluation period in seconds. The evaluation window describes for how long the average cluster CPU utilization should stay above the threshold for it to be considered as eligible for scale up. You can edit settings for this policy via the CAST AI console : or the CAST AI policies API endpoint by setting values for \"cpuUtilization\" : { \"scaleUpThreshold\" : { \"avgCpuLoadPercentageMoreThan\" : <value> , \"enabled\" : <value> , \"evaluationPeriodSeconds\" : <value> } } It may take a few minutes for the new settings to propagate.","title":"Configuring the CPU utilization scale up policy"},{"location":"guides/autoscaling-policies/#policies-precedence-rules","text":"If multiple policies are enabled and multiple rules are triggered during the same evaluation period, they will be handled in the following order: Cluster CPU limits policy Horizontal Pod Autoscaler (HPA) policy Unscheduled pods policy Cluster CPU utilization scale up policy","title":"Policies precedence rules"},{"location":"guides/ingress/","text":"Exposing your app to the internet \u00b6 Making your CAST AI hosted application available on the internet is done in the conventional Kubernetes way: by deploying an ingress . CAST AI clusters are automatically provisioned with: Ingress controller and the necessary multi-cloud load balancers infrastructure; A certificate manager configured to manage TLS certificates with letsencrypt.org ; Metric collection for your ingress traffic; See architecture overview for more details. Let's deploy, configure, and inspect a basic application: an empty Caddy server. Prerequisites \u00b6 First and foremost, you need to create or have a CAST AI cluster ( guide ) ready to go. On the cluster details page in the console, note the \"GSLB DNS\" value. The value should look similar to 1234567890.your-cluster-name-7da6f229.onmulti.cloud once the cluster is done creating. This is the internal DNS name for your future ingress. But for the TLS setup to work, you'll also need an CNAME alias for it, using host name of your choice. For example, if you prepare to serve your application on https://sample-app.yourdomain.com , create a CNAME record in your DNS provider with the name sample-app and value 1234567890.your-cluster-name-7da6f229.onmulti.cloud . Note If you check the DNS resolution at this point, e.g. dig sample-app.yourdomain.com , you should be able to see that the name resolves to one or more cloud-specific load balancers. Deployment \u00b6 It's a rather bare-bones setup consisting of 2-replica deployment, a service description for that deployment, and an ingress resource to publish that service. Change value sample-app.yourdomain.com to the DNS CNAME that you created before, and deploy everything else as-is to your cluster. apiVersion : apps/v1 kind : Deployment metadata : name : sample-app spec : replicas : 2 selector : matchLabels : app : sample-app template : metadata : labels : app : sample-app spec : containers : - name : sample-app image : caddy:2.2.1-alpine ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : sample-app spec : type : NodePort selector : app : sample-app ports : - name : http port : 80 targetPort : 80 --- apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : sample-app-ingress annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/cluster-issuer : \"letsencrypt-prod\" spec : tls : - hosts : - sample-app.yourdomain.com secretName : sample-app rules : - host : sample-app.yourdomain.com http : paths : - path : / backend : serviceName : sample-app servicePort : http Verification \u00b6 After deploying the configuration above, the application should be ready for testing in a few moments. Check in the browser or CLI, e.g.: $ curl -L -I sample-app.yourdomain.com HTTP/1.1 308 Permanent Redirect Date: Wed, 13 Jan 2021 11:30:52 GMT Content-Type: text/html Content-Length: 164 Connection: keep-alive Location: https://sample-app.yourdomain.com/ HTTP/2 200 date: Wed, 13 Jan 2021 11:30:52 GMT content-type: text/html; charset=utf-8 content-length: 12226 vary: Accept-Encoding accept-ranges: bytes etag: \"qlhhn49fm\" last-modified: Thu, 17 Dec 2020 12:35:28 GMT strict-transport-security: max-age=15724800; includeSubDomains You can see a few things happening here: HTTP->HTTPS redirect is established automatically; Once redirected to HTTPS, your application TLS setup works properly (curl is able to verify certificate validity for your domain). Deployment without CNAME alias \u00b6 If you skipped the DNS setup until this point, you should still be able to ping your application and get a response back. The only difference is that TLS certificate will not be provisioned, as certificate manager can't complete a HTTP-01 challenge without LetsEncrypt being able to reach your app via the \"official\" URL. To ping our app without a DNS CNAME, use the internal DNS name and pass \"host\" header for the ingress routing to work. You'll need to ignore certificate errors, as your application will be using self-signed certificate as a fallback. $ curl -s -k -H \"Host: sample-app.yourdomain.com\" https://1234567890.your-cluster-name-7da6f229.onmulti.cloud | head -n 4 <!DOCTYPE html> <html> <head> <title>Caddy works!</title> If you don't intend creating a user-friendly url, another alternative is to use internal DNS name as ingress host. This will enable cert manager to provision proper TLS certificate and your app will be reachable via this name directly. spec : tls : - hosts : - 1234567890.your-cluster-name-7da6f229.onmulti.cloud secretName : sample-app rules : - host : 1234567890.your-cluster-name-7da6f229.onmulti.cloud http : Metrics \u00b6 Once you have your application up and running, you can check another out-of-the-box feature CAST AI configures for you: the ingress metrics and dashboard. Head to CAST.AI console, and in your cluster details page, click on the \"Grafana metrics\" link in the side menu. Once in Grafana, click \"Home\" in the top-left corner and open \"NGINX Ingress controller\" dashboard. You should be greeted with a view similar to this: This dashboard provides an overview of your application traffic. To tailor the dashboard to your specific needs, refer to NGINX metrics documentation for more details on available metrics. Combinations \u00b6 Single host, multiple services \u00b6 You can use path-based routing to redirect traffic to specific services using ingress rule paths : spec : rules : - host : sample-app.yourdomain.com http : paths : - path : /static backend : serviceName : static-resources servicePort : http - path : / backend : serviceName : base-app servicePort : http Multiple hosts \u00b6 To manage multiple domains, you can just deploy multiple ingress resources, or include more domains into same ingress resource. # first host apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : sample-app-ingress annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/cluster-issuer : \"letsencrypt-prod\" spec : tls : - hosts : - sample-app.yourdomain.com secretName : sample-app-cert rules : - host : sample-app.yourdomain.com http : paths : - path : / backend : serviceName : sample-app servicePort : http --- # second host apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : sample-app-ingress2 annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/cluster-issuer : \"letsencrypt-prod\" spec : tls : - hosts : - sample-app2.yourdomain.com # note that secret needs to be unique for each domain, unless deployments # will be separated by kubernetes namespaces secretName : sample-app-cert-2 rules : - host : sample-app2.yourdomain.com http : paths : - path : / backend : serviceName : sample-app2 servicePort : http --- # combining: multiple hosts per certificate and/or multiple certificates per single ingress resource apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : sample-app-ingress3 annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/cluster-issuer : \"letsencrypt-prod\" spec : tls : - hosts : # two domains under single certificate - sample-app3.yourdomain.com - sample-app3-alternative.yourdomain.com secretName : sample-app3-cert - hosts : # another side-by-side certificate - sample-app4.yourdomain.com secretName : sample-app4-cert rules : - host : sample-app3.yourdomain.com http : paths : - path : / backend : serviceName : sample-app servicePort : http - host : sample-app3-alternative.yourdomain.com http : paths : - path : / backend : serviceName : sample-app servicePort : http - host : sample-app4.yourdomain.com http : paths : - path : / backend : serviceName : sample-app2 servicePort : http","title":"Exposing your app to the internet"},{"location":"guides/ingress/#exposing-your-app-to-the-internet","text":"Making your CAST AI hosted application available on the internet is done in the conventional Kubernetes way: by deploying an ingress . CAST AI clusters are automatically provisioned with: Ingress controller and the necessary multi-cloud load balancers infrastructure; A certificate manager configured to manage TLS certificates with letsencrypt.org ; Metric collection for your ingress traffic; See architecture overview for more details. Let's deploy, configure, and inspect a basic application: an empty Caddy server.","title":"Exposing your app to the internet"},{"location":"guides/ingress/#prerequisites","text":"First and foremost, you need to create or have a CAST AI cluster ( guide ) ready to go. On the cluster details page in the console, note the \"GSLB DNS\" value. The value should look similar to 1234567890.your-cluster-name-7da6f229.onmulti.cloud once the cluster is done creating. This is the internal DNS name for your future ingress. But for the TLS setup to work, you'll also need an CNAME alias for it, using host name of your choice. For example, if you prepare to serve your application on https://sample-app.yourdomain.com , create a CNAME record in your DNS provider with the name sample-app and value 1234567890.your-cluster-name-7da6f229.onmulti.cloud . Note If you check the DNS resolution at this point, e.g. dig sample-app.yourdomain.com , you should be able to see that the name resolves to one or more cloud-specific load balancers.","title":"Prerequisites"},{"location":"guides/ingress/#deployment","text":"It's a rather bare-bones setup consisting of 2-replica deployment, a service description for that deployment, and an ingress resource to publish that service. Change value sample-app.yourdomain.com to the DNS CNAME that you created before, and deploy everything else as-is to your cluster. apiVersion : apps/v1 kind : Deployment metadata : name : sample-app spec : replicas : 2 selector : matchLabels : app : sample-app template : metadata : labels : app : sample-app spec : containers : - name : sample-app image : caddy:2.2.1-alpine ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : sample-app spec : type : NodePort selector : app : sample-app ports : - name : http port : 80 targetPort : 80 --- apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : sample-app-ingress annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/cluster-issuer : \"letsencrypt-prod\" spec : tls : - hosts : - sample-app.yourdomain.com secretName : sample-app rules : - host : sample-app.yourdomain.com http : paths : - path : / backend : serviceName : sample-app servicePort : http","title":"Deployment"},{"location":"guides/ingress/#verification","text":"After deploying the configuration above, the application should be ready for testing in a few moments. Check in the browser or CLI, e.g.: $ curl -L -I sample-app.yourdomain.com HTTP/1.1 308 Permanent Redirect Date: Wed, 13 Jan 2021 11:30:52 GMT Content-Type: text/html Content-Length: 164 Connection: keep-alive Location: https://sample-app.yourdomain.com/ HTTP/2 200 date: Wed, 13 Jan 2021 11:30:52 GMT content-type: text/html; charset=utf-8 content-length: 12226 vary: Accept-Encoding accept-ranges: bytes etag: \"qlhhn49fm\" last-modified: Thu, 17 Dec 2020 12:35:28 GMT strict-transport-security: max-age=15724800; includeSubDomains You can see a few things happening here: HTTP->HTTPS redirect is established automatically; Once redirected to HTTPS, your application TLS setup works properly (curl is able to verify certificate validity for your domain).","title":"Verification"},{"location":"guides/ingress/#deployment-without-cname-alias","text":"If you skipped the DNS setup until this point, you should still be able to ping your application and get a response back. The only difference is that TLS certificate will not be provisioned, as certificate manager can't complete a HTTP-01 challenge without LetsEncrypt being able to reach your app via the \"official\" URL. To ping our app without a DNS CNAME, use the internal DNS name and pass \"host\" header for the ingress routing to work. You'll need to ignore certificate errors, as your application will be using self-signed certificate as a fallback. $ curl -s -k -H \"Host: sample-app.yourdomain.com\" https://1234567890.your-cluster-name-7da6f229.onmulti.cloud | head -n 4 <!DOCTYPE html> <html> <head> <title>Caddy works!</title> If you don't intend creating a user-friendly url, another alternative is to use internal DNS name as ingress host. This will enable cert manager to provision proper TLS certificate and your app will be reachable via this name directly. spec : tls : - hosts : - 1234567890.your-cluster-name-7da6f229.onmulti.cloud secretName : sample-app rules : - host : 1234567890.your-cluster-name-7da6f229.onmulti.cloud http :","title":"Deployment without CNAME alias"},{"location":"guides/ingress/#metrics","text":"Once you have your application up and running, you can check another out-of-the-box feature CAST AI configures for you: the ingress metrics and dashboard. Head to CAST.AI console, and in your cluster details page, click on the \"Grafana metrics\" link in the side menu. Once in Grafana, click \"Home\" in the top-left corner and open \"NGINX Ingress controller\" dashboard. You should be greeted with a view similar to this: This dashboard provides an overview of your application traffic. To tailor the dashboard to your specific needs, refer to NGINX metrics documentation for more details on available metrics.","title":"Metrics"},{"location":"guides/ingress/#combinations","text":"","title":"Combinations"},{"location":"guides/ingress/#single-host-multiple-services","text":"You can use path-based routing to redirect traffic to specific services using ingress rule paths : spec : rules : - host : sample-app.yourdomain.com http : paths : - path : /static backend : serviceName : static-resources servicePort : http - path : / backend : serviceName : base-app servicePort : http","title":"Single host, multiple services"},{"location":"guides/ingress/#multiple-hosts","text":"To manage multiple domains, you can just deploy multiple ingress resources, or include more domains into same ingress resource. # first host apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : sample-app-ingress annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/cluster-issuer : \"letsencrypt-prod\" spec : tls : - hosts : - sample-app.yourdomain.com secretName : sample-app-cert rules : - host : sample-app.yourdomain.com http : paths : - path : / backend : serviceName : sample-app servicePort : http --- # second host apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : sample-app-ingress2 annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/cluster-issuer : \"letsencrypt-prod\" spec : tls : - hosts : - sample-app2.yourdomain.com # note that secret needs to be unique for each domain, unless deployments # will be separated by kubernetes namespaces secretName : sample-app-cert-2 rules : - host : sample-app2.yourdomain.com http : paths : - path : / backend : serviceName : sample-app2 servicePort : http --- # combining: multiple hosts per certificate and/or multiple certificates per single ingress resource apiVersion : networking.k8s.io/v1beta1 kind : Ingress metadata : name : sample-app-ingress3 annotations : kubernetes.io/ingress.class : \"nginx\" cert-manager.io/cluster-issuer : \"letsencrypt-prod\" spec : tls : - hosts : # two domains under single certificate - sample-app3.yourdomain.com - sample-app3-alternative.yourdomain.com secretName : sample-app3-cert - hosts : # another side-by-side certificate - sample-app4.yourdomain.com secretName : sample-app4-cert rules : - host : sample-app3.yourdomain.com http : paths : - path : / backend : serviceName : sample-app servicePort : http - host : sample-app3-alternative.yourdomain.com http : paths : - path : / backend : serviceName : sample-app servicePort : http - host : sample-app4.yourdomain.com http : paths : - path : / backend : serviceName : sample-app2 servicePort : http","title":"Multiple hosts"},{"location":"guides/pod-pinning/","text":"Configure pod placement by topology \u00b6 A user of CAST AI might want to place their pods only on a particular cloud or clouds (for example, on AWS and GCP, but not Azure). Kubernetes supports this by using nodeSelector , node Affinity/Anti-Affinity and topologySpreadConstraints . All of these methods require special labels to be present on each Kubernetes node. CAST AI multi-cloud Kubernetes cluster nodes are already equipped with the following labels: Label Type Description Example(s) node.kubernetes.io/instance-type well-known Node type (cloud-specific) t3a.large, e2-standard-4 kubernetes.io/arch well-known Node CPU architecture amd64 kubernetes.io/hostname well-known Node Hostname ip-10-10-2-81, testcluster-31qd-gcp-3ead kubernetes.io/os well-known Node Operating System linux topology.kubernetes.io/region well-known Node region in the CSP eu-central-1 topology.kubernetes.io/zone well-known Node zone of the region in the CSP eu-central-1a topology.cast.ai/csp cast-specific Node Cloud Service Provider aws, gcp, azure How to pin a pod to AWS \u00b6 We will use affinity.nodeAffinity : affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: topology.cast.ai/csp operator: In values: - aws Pod example: apiVersion: v1 kind: Pod metadata: name: nginx-pod labels: app: nginx spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: topology.storage.csi.cast.ai/csp operator: In values: - aws containers: - name: nginx image: k8s.gcr.io/nginx-slim:0.8 ports: - containerPort: 80 name: web StatefulSet example, it will create 3 pods each in every cloud (note the podAntiAffinity) apiVersion: apps/v1 kind: StatefulSet metadata: name: a-web spec: podManagementPolicy: Parallel serviceName: \"nginx\" replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: topology.storage.csi.cast.ai/csp operator: In values: - aws - gcp - azure podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - nginx topologyKey: topology.storage.csi.cast.ai/csp containers: - name: nginx image: k8s.gcr.io/nginx-slim:0.8 ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: [ \"ReadWriteOnce\" ] resources: requests: storage: 1Gi","title":"Configure pod placement by topology"},{"location":"guides/pod-pinning/#configure-pod-placement-by-topology","text":"A user of CAST AI might want to place their pods only on a particular cloud or clouds (for example, on AWS and GCP, but not Azure). Kubernetes supports this by using nodeSelector , node Affinity/Anti-Affinity and topologySpreadConstraints . All of these methods require special labels to be present on each Kubernetes node. CAST AI multi-cloud Kubernetes cluster nodes are already equipped with the following labels: Label Type Description Example(s) node.kubernetes.io/instance-type well-known Node type (cloud-specific) t3a.large, e2-standard-4 kubernetes.io/arch well-known Node CPU architecture amd64 kubernetes.io/hostname well-known Node Hostname ip-10-10-2-81, testcluster-31qd-gcp-3ead kubernetes.io/os well-known Node Operating System linux topology.kubernetes.io/region well-known Node region in the CSP eu-central-1 topology.kubernetes.io/zone well-known Node zone of the region in the CSP eu-central-1a topology.cast.ai/csp cast-specific Node Cloud Service Provider aws, gcp, azure","title":"Configure pod placement by topology"},{"location":"guides/pod-pinning/#how-to-pin-a-pod-to-aws","text":"We will use affinity.nodeAffinity : affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: topology.cast.ai/csp operator: In values: - aws Pod example: apiVersion: v1 kind: Pod metadata: name: nginx-pod labels: app: nginx spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: topology.storage.csi.cast.ai/csp operator: In values: - aws containers: - name: nginx image: k8s.gcr.io/nginx-slim:0.8 ports: - containerPort: 80 name: web StatefulSet example, it will create 3 pods each in every cloud (note the podAntiAffinity) apiVersion: apps/v1 kind: StatefulSet metadata: name: a-web spec: podManagementPolicy: Parallel serviceName: \"nginx\" replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: topology.storage.csi.cast.ai/csp operator: In values: - aws - gcp - azure podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - nginx topologyKey: topology.storage.csi.cast.ai/csp containers: - name: nginx image: k8s.gcr.io/nginx-slim:0.8 ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: [ \"ReadWriteOnce\" ] resources: requests: storage: 1Gi","title":"How to pin a pod to AWS"},{"location":"guides/volumes/","text":"Dynamic volume provisioning \u00b6 Dynamic volume provisioning allows storage volumes to be created on-demand. Without dynamic provisioning, cluster administrators have to create new storage volumes manually (using cloud or storage providers) and the corresponding PersistentVolume objects for the storage to be available in Kubernetes. Dynamic volume provisioning is enabled by default on CAST AI cluster. Overview \u00b6 Each CAST AI cluster is pre-configured with default StorageClass that handles volume requests. \u00bb kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE cast-block-storage ( default ) storage.csi.cast.ai Delete WaitForFirstConsumer true 2m18s The binding mode WaitForFirstConsumer will delay the binding and provisioning of a PersistentVolume until a Pod using the PVC is created. Meaning, the volume will be created and attached to the Node on which a Pod using the PVC will be run. In the case of a Pod replicated across multiple clouds, volumes will be distributed across clouds as well. This will limit Pod scheduling only to the nodes of the same cloud since to reschedule a Pod to a different cloud service, the volume must be replicated to that cloud. This limitation will be removed by the cross-cloud volume replication feature which isn't available at the moment. Deleting a cluster will delete all the volumes that were provisioned dynamically. Using dynamic volumes \u00b6 Creating persitent volume claim (PVC) \u00b6 Users can request dynamically provisioned storage by simply creating PersistentVolumeClaim and a Pod that will use it. apiVersion : v1 kind : PersistentVolumeClaim metadata : name : example-claim spec : accessModes : - ReadWriteOnce resources : requests : storage : 50Gi Pod example: apiVersion : v1 kind : Pod metadata : name : app spec : containers : - name : app image : centos command : [ \"/bin/sh\" ] args : [ \"-c\" , \"while true; do echo $(date -u) >> /data/out.txt; sleep 5; done\" ] volumeMounts : - name : persistent-storage mountPath : /data volumes : - name : persistent-storage persistentVolumeClaim : claimName : example-claim This claim results in an Persistent Disk being automatically provisioned. When the claim is deleted, the volume is deleted as well. Volume claim templates \u00b6 Additionally, having StatefulSet user can define volumeClaimTemplates to provision volumes without creating PVC beforehand. apiVersion : v1 kind : Service metadata : name : nginx labels : app : nginx spec : ports : - port : 80 name : web clusterIP : None selector : app : nginx --- apiVersion : apps/v1 kind : StatefulSet metadata : name : web spec : selector : matchLabels : app : nginx serviceName : \"nginx\" replicas : 3 template : metadata : labels : app : nginx spec : containers : - name : nginx image : k8s.gcr.io/nginx-slim:0.8 ports : - containerPort : 80 name : web volumeMounts : - name : www mountPath : /usr/share/nginx/html volumeClaimTemplates : - metadata : name : www spec : accessModes : [ \"ReadWriteOnce\" ] resources : requests : storage : 1Gi This will result in dynamic PVC for each StatefulSet pod. \u00bb kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE www-web-0 Bound pvc-3b550fd0-4b79-449d-b1cf-f51264f975fc 1Gi RWO cast-block-storage 3m11s www-web-1 Bound pvc-0c7470a8-cc59-49d4-b2ca-5d3db45c1b60 1Gi RWO cast-block-storage 2m41s www-web-2 Bound pvc-70c341cc-fa1c-471a-882a-e46225e1824f 1Gi RWO cast-block-storage 2m18s Deleting a StatefulSet will delete all provisioned volumes. Resizing PVC \u00b6 Any PVC created using cast-block-storage StorageClass can be edited to request more space. Kubernetes will interpret a change to the storage field as a request for more space. This will trigger automatic volume resizing. \u00bb kubectl edit pvc www-web-0 Change storage field as shown below: # www-web-0... spec : accessModes : - ReadWriteOnce resources : requests : storage : 10Gi # new storage size storageClassName : cast-block-storage # www-web-0... After storage is resized successfully, we can observe new PVC capacity: k get pvc www-web-0 NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE www-web-0 Bound pvc-edd59e56-cb22-41b6-a075-ab8820f222b8 10Gi RWO cast-block-storage 4m57s","title":"Dynamic volume provisioning"},{"location":"guides/volumes/#dynamic-volume-provisioning","text":"Dynamic volume provisioning allows storage volumes to be created on-demand. Without dynamic provisioning, cluster administrators have to create new storage volumes manually (using cloud or storage providers) and the corresponding PersistentVolume objects for the storage to be available in Kubernetes. Dynamic volume provisioning is enabled by default on CAST AI cluster.","title":"Dynamic volume provisioning"},{"location":"guides/volumes/#overview","text":"Each CAST AI cluster is pre-configured with default StorageClass that handles volume requests. \u00bb kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE cast-block-storage ( default ) storage.csi.cast.ai Delete WaitForFirstConsumer true 2m18s The binding mode WaitForFirstConsumer will delay the binding and provisioning of a PersistentVolume until a Pod using the PVC is created. Meaning, the volume will be created and attached to the Node on which a Pod using the PVC will be run. In the case of a Pod replicated across multiple clouds, volumes will be distributed across clouds as well. This will limit Pod scheduling only to the nodes of the same cloud since to reschedule a Pod to a different cloud service, the volume must be replicated to that cloud. This limitation will be removed by the cross-cloud volume replication feature which isn't available at the moment. Deleting a cluster will delete all the volumes that were provisioned dynamically.","title":"Overview"},{"location":"guides/volumes/#using-dynamic-volumes","text":"","title":"Using dynamic volumes"},{"location":"guides/volumes/#creating-persitent-volume-claim-pvc","text":"Users can request dynamically provisioned storage by simply creating PersistentVolumeClaim and a Pod that will use it. apiVersion : v1 kind : PersistentVolumeClaim metadata : name : example-claim spec : accessModes : - ReadWriteOnce resources : requests : storage : 50Gi Pod example: apiVersion : v1 kind : Pod metadata : name : app spec : containers : - name : app image : centos command : [ \"/bin/sh\" ] args : [ \"-c\" , \"while true; do echo $(date -u) >> /data/out.txt; sleep 5; done\" ] volumeMounts : - name : persistent-storage mountPath : /data volumes : - name : persistent-storage persistentVolumeClaim : claimName : example-claim This claim results in an Persistent Disk being automatically provisioned. When the claim is deleted, the volume is deleted as well.","title":"Creating persitent volume claim (PVC)"},{"location":"guides/volumes/#volume-claim-templates","text":"Additionally, having StatefulSet user can define volumeClaimTemplates to provision volumes without creating PVC beforehand. apiVersion : v1 kind : Service metadata : name : nginx labels : app : nginx spec : ports : - port : 80 name : web clusterIP : None selector : app : nginx --- apiVersion : apps/v1 kind : StatefulSet metadata : name : web spec : selector : matchLabels : app : nginx serviceName : \"nginx\" replicas : 3 template : metadata : labels : app : nginx spec : containers : - name : nginx image : k8s.gcr.io/nginx-slim:0.8 ports : - containerPort : 80 name : web volumeMounts : - name : www mountPath : /usr/share/nginx/html volumeClaimTemplates : - metadata : name : www spec : accessModes : [ \"ReadWriteOnce\" ] resources : requests : storage : 1Gi This will result in dynamic PVC for each StatefulSet pod. \u00bb kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE www-web-0 Bound pvc-3b550fd0-4b79-449d-b1cf-f51264f975fc 1Gi RWO cast-block-storage 3m11s www-web-1 Bound pvc-0c7470a8-cc59-49d4-b2ca-5d3db45c1b60 1Gi RWO cast-block-storage 2m41s www-web-2 Bound pvc-70c341cc-fa1c-471a-882a-e46225e1824f 1Gi RWO cast-block-storage 2m18s Deleting a StatefulSet will delete all provisioned volumes.","title":"Volume claim templates"},{"location":"guides/volumes/#resizing-pvc","text":"Any PVC created using cast-block-storage StorageClass can be edited to request more space. Kubernetes will interpret a change to the storage field as a request for more space. This will trigger automatic volume resizing. \u00bb kubectl edit pvc www-web-0 Change storage field as shown below: # www-web-0... spec : accessModes : - ReadWriteOnce resources : requests : storage : 10Gi # new storage size storageClassName : cast-block-storage # www-web-0... After storage is resized successfully, we can observe new PVC capacity: k get pvc www-web-0 NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE www-web-0 Bound pvc-edd59e56-cb22-41b6-a075-ab8820f222b8 10Gi RWO cast-block-storage 4m57s","title":"Resizing PVC"},{"location":"guides/node-autoscaler/spot-instances/spot/","text":"Spot/Preemptible Instances \u00b6 The CAST AI autoscaler supports running your workloads on Spot/Preemtible instances. In this guide, we will show you just how easy it is to do that. Available configurations \u00b6 Tolerations \u00b6 When a pod is marked only with Toleration, the Kubernetes scheduler could place such a pod/pods on regular nodes as well. This option should be preferred when spot instances are optional for your workloads. ... tolerations : - key : scheduling.cast.ai/spot operator : Exists ... Node Selectors \u00b6 If you want to make sure that a pod is scheduled on spot instances only, in addition to tolerations, you must add nodeSelector as well. That way, the autoscaler ensures that whenever your pod requires additional workload in the cluster, only a spot instance is picked to satisfy the needs. ... tolerations : - key : scheduling.cast.ai/spot operator : Exists nodeSelector : scheduling.cast.ai/spot : \"true\" ... Step-by-step deployment on Spot Instance \u00b6 In this step-by-step guide, we demonstrate how to use Spot Instances with your CAST AI clusters. To do that, we will be using example NGINX deployment configured to only be ran on Spot/Preemtible instances. 0. Pre-requisites \u00b6 Have a Kubernetes cluster on CAST AI Check Creating your first cluster if you need guidance. Kubeconfig downloaded and ready to use for deploying an example application to your cluster. 1. Enable relevant policies \u00b6 To get started on using Spot instances autoscaler, you should enable two policies under Policies configuration in the UI: Spot/Preemptible instances policy This policy allows the autoscaler to use spot instances Unschedulable pods policy This policy requests an additional workload to be scheduled based on your deployment requires (i.e. run on spot instances) 2. Example deployment \u00b6 Save the following yaml file, and name it: nginx.yaml : apiVersion : apps/v1 kind : Deployment metadata : name : nginx-deployment labels : app : nginx spec : replicas : 1 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : nodeSelector : scheduling.cast.ai/spot : \"true\" topology.cast.ai/csp : \"aws\" tolerations : - key : scheduling.cast.ai/spot operator : Exists containers : - name : nginx image : nginx:1.14.2 ports : - containerPort : 80 resources : requests : cpu : '2' limits : cpu : '3' 2.1. Apply the example deployment \u00b6 With Kubeconfig set in your current shell session, you can execute the following (or use other means of applying deployment files): kubectl apply -f ngninx.yaml 2.2. Wait several minutes \u00b6 Once the deployment is created, it will take up to several minutes for the autoscaler to pick up the information about your pending deployment and schedule the relevant workloads in order to satisfy the deployment needs, such as: This deployment tolerates spot instances This deployment must run only on spot instances 3. Spot Instance added \u00b6 You can see your newly added spot instance in the cluster node list. 3.1. AWS instance list \u00b6 Just to double check, go to AWS console and check that the added node has the Lifecycle: spot indicator.","title":"Spot/Preemptible Instances"},{"location":"guides/node-autoscaler/spot-instances/spot/#spotpreemptible-instances","text":"The CAST AI autoscaler supports running your workloads on Spot/Preemtible instances. In this guide, we will show you just how easy it is to do that.","title":"Spot/Preemptible Instances"},{"location":"guides/node-autoscaler/spot-instances/spot/#available-configurations","text":"","title":"Available configurations"},{"location":"guides/node-autoscaler/spot-instances/spot/#tolerations","text":"When a pod is marked only with Toleration, the Kubernetes scheduler could place such a pod/pods on regular nodes as well. This option should be preferred when spot instances are optional for your workloads. ... tolerations : - key : scheduling.cast.ai/spot operator : Exists ...","title":"Tolerations"},{"location":"guides/node-autoscaler/spot-instances/spot/#node-selectors","text":"If you want to make sure that a pod is scheduled on spot instances only, in addition to tolerations, you must add nodeSelector as well. That way, the autoscaler ensures that whenever your pod requires additional workload in the cluster, only a spot instance is picked to satisfy the needs. ... tolerations : - key : scheduling.cast.ai/spot operator : Exists nodeSelector : scheduling.cast.ai/spot : \"true\" ...","title":"Node Selectors"},{"location":"guides/node-autoscaler/spot-instances/spot/#step-by-step-deployment-on-spot-instance","text":"In this step-by-step guide, we demonstrate how to use Spot Instances with your CAST AI clusters. To do that, we will be using example NGINX deployment configured to only be ran on Spot/Preemtible instances.","title":"Step-by-step deployment on Spot Instance"},{"location":"guides/node-autoscaler/spot-instances/spot/#0-pre-requisites","text":"Have a Kubernetes cluster on CAST AI Check Creating your first cluster if you need guidance. Kubeconfig downloaded and ready to use for deploying an example application to your cluster.","title":"0. Pre-requisites"},{"location":"guides/node-autoscaler/spot-instances/spot/#1-enable-relevant-policies","text":"To get started on using Spot instances autoscaler, you should enable two policies under Policies configuration in the UI: Spot/Preemptible instances policy This policy allows the autoscaler to use spot instances Unschedulable pods policy This policy requests an additional workload to be scheduled based on your deployment requires (i.e. run on spot instances)","title":"1. Enable relevant policies"},{"location":"guides/node-autoscaler/spot-instances/spot/#2-example-deployment","text":"Save the following yaml file, and name it: nginx.yaml : apiVersion : apps/v1 kind : Deployment metadata : name : nginx-deployment labels : app : nginx spec : replicas : 1 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : nodeSelector : scheduling.cast.ai/spot : \"true\" topology.cast.ai/csp : \"aws\" tolerations : - key : scheduling.cast.ai/spot operator : Exists containers : - name : nginx image : nginx:1.14.2 ports : - containerPort : 80 resources : requests : cpu : '2' limits : cpu : '3'","title":"2. Example deployment"},{"location":"guides/node-autoscaler/spot-instances/spot/#21-apply-the-example-deployment","text":"With Kubeconfig set in your current shell session, you can execute the following (or use other means of applying deployment files): kubectl apply -f ngninx.yaml","title":"2.1. Apply the example deployment"},{"location":"guides/node-autoscaler/spot-instances/spot/#22-wait-several-minutes","text":"Once the deployment is created, it will take up to several minutes for the autoscaler to pick up the information about your pending deployment and schedule the relevant workloads in order to satisfy the deployment needs, such as: This deployment tolerates spot instances This deployment must run only on spot instances","title":"2.2. Wait several minutes"},{"location":"guides/node-autoscaler/spot-instances/spot/#3-spot-instance-added","text":"You can see your newly added spot instance in the cluster node list.","title":"3. Spot Instance added"},{"location":"guides/node-autoscaler/spot-instances/spot/#31-aws-instance-list","text":"Just to double check, go to AWS console and check that the added node has the Lifecycle: spot indicator.","title":"3.1. AWS instance list"},{"location":"guides/pod-autoscaler/hpa/","text":"Horizontal Pod Autoscaler \u00b6 Scaling an application \u00b6 You can scale an application in two ways: Vertically: by adding more resources (RAM/CPU/Disk IOPS) to the same instance, Horizontally: by adding more instances (replicas) of the same application. The problem with vertical scaling is that you'll either find out that the required hardware (RAM, CPU, Disk IOPS) in a single machine costs too much, or the cloud provider cannot provision a machine with enough resources. We use replica sets in Kubernetes to achieve horizontal scaling. The Horizontal Pod Autoscaler allows automating the process of maintaining the replica count proportionally to the application load. Horizontal scaling strategy \u00b6 As mentioned above, the horizontal scaling strategy involves adding (or removing) the additional replicas of the same application. The problem here lies in the fact that most application load patterns can have spikes that are not predictable. This renders manual scaling nearly impossible. Luckily, we can automate this process! The HPA & KEDA \u00b6 Kubernetes comes equipped with the Horizontal Pod Autoscaler (HPA) functionality. It can scale up (add more replicas) or down (remove idling replicas) based on some metrics. The problem is that HPA, by default, comes without batteries: it doesn't have the metrics' source. But you're in luck: CAST AI bundles the batteries for you! We've got you covered by providing the KEDA addon. How does it work \u00b6 KEDA consists of two components: operator -- watches k8s for ScaledObject resources and configures HPA accordingly metrics-apiserver -- a bridge between Kubernetes and various scaling sources (including Prometheus) These components do the heavy lifting of configuring Kubernetes HPA and setting up the custom metric sources. This enables us to autoscale almost any workload: Deployment , ReplicaSet , ReplicationController , or StatefulSet . KEDA does support autoscaling Jobs as well. Enabling KEDA \u00b6 In order to take advantage of the autoscaling functionality, you must enable KEDA addon in the Policies page: Navigate to an existing cluster (in case you don't have one already, go ahead and create one ) On the left navigation menu, select Policies : Enable the Horizontal pod autoscaler Examples \u00b6 Autoscale Based on CPU and/or Memory usage \u00b6 Let's create a Deployment and a Service that we will Autoscale : apiVersion : apps/v1 kind : Deployment metadata : name : sample-app labels : app : sample-app spec : # Note that we omit the replica count so # when we redeploy, we wouldn't override # replica count set by the autoscaler #replicas: 1 selector : matchLabels : app : sample-app template : metadata : labels : app : sample-app spec : containers : - image : luxas/autoscale-demo:v0.1.2 name : sample-app ports : - containerPort : 8080 --- apiVersion : v1 kind : Service metadata : name : sample-app labels : app : sample-app spec : ports : - port : 8080 name : http targetPort : 8080 protocol : TCP selector : app : sample-app Note : We don't specify the ReplicaCount ourselves Now let us set up a CPU-based Autoscaler apiVersion : keda.sh/v1alpha1 kind : ScaledObject metadata : name : sample-app spec : scaleTargetRef : name : sample-app minReplicaCount : 1 # Optional. Default: 0 maxReplicaCount : 10 # Optional. Default: 100 triggers : # Either of the triggers can be omitted. - type : cpu metadata : # Possible values: `Value`, `Utilization`, or `AverageValue`. # More info at: https://keda.sh/docs/2.0/scalers/cpu/#trigger-specification type : \"Value\" value : \"30\" - type : memory metadata : # Possible values: `Value`, `Utilization`, or `AverageValue`. # More info at: https://keda.sh/docs/2.0/scalers/memory/ type : \"Value\" value : \"512\" Now our Deployment autoscaling will be triggered either by CPU or Memory usage. We could use any other trigger, or remove either of those if we want (i.e. to autoscale only on the CPU basis, remove the Memory trigger, and vice-versa). Autoscale based on the Prometheus metric \u00b6 It's possible to autoscale based on the result of an arbitrary Prometheus query. What's more, CAST AI k8s clusters come with Prometheus deployed out of the box! Let's deploy the sample app again. But this time, let's instruct Prometheus to scrape metrics: apiVersion : apps/v1 kind : Deployment metadata : name : sample-app labels : app : sample-app spec : selector : matchLabels : app : sample-app template : metadata : labels : app : sample-app annotations : # These annotations the main difference! prometheus.io/path : \"/metrics\" prometheus.io/port : \"8080\" prometheus.io/scrape : \"true\" spec : containers : - image : luxas/autoscale-demo:v0.1.2 name : sample-app ports : - containerPort : 8080 --- apiVersion : v1 kind : Service metadata : name : sample-app labels : app : sample-app spec : ports : - port : 8080 name : http targetPort : 8080 protocol : TCP selector : app : sample-app Now let's deploy the Autoscaler! apiVersion : keda.sh/v1alpha1 kind : ScaledObject metadata : name : sample-app spec : scaleTargetRef : name : sample-app minReplicaCount : 1 # Optional. Default: 0 maxReplicaCount : 10 # Optional. Default: 100 triggers : - type : prometheus metadata : serverAddress : http://prom.castai:9090 metricName : http_requests_total_sample_app threshold : '1' # Note: query must return a vector/scalar single element response query : sum(rate(http_requests_total{app=\"sample-app\"}[2m])) Now let's generate some load and observe that the replica count is increased: # Deploy busybox image kubectl run -it --rm load-generator --image = busybox /bin/sh # Hit ENTER for command prompt # trigger infinite requests to the php-apache server while true ; do wget -q -O- http://sample-app:8080/metrics ; done # in order to cancel, hold CTRL+C # in order to quit, initiate CTRL+D sequence Troubleshooting \u00b6 Verify that KEDA is scheduled and running (the suffixes might be different): $ kubectl get pods -n keda NAME READY STATUS RESTARTS AGE keda-metrics-apiserver-59679c9f96-5lfr5 1 /1 Running 0 74m keda-operator-66744fc69d-7njdd 1 /1 Running 0 74m Describe ScaledObject for clues. In this case, scaledObjectRef points to nonexistent object: $ kubectl describe scaledobjects.keda.sh sample-app Name: sample-app Namespace: default Labels: scaledObjectName = sample-app Annotations: API Version: keda.sh/v1alpha1 Kind: ScaledObject Metadata: Creation Timestamp: 2020 -11-10T10:12:38Z Finalizers: finalizer.keda.sh Generation: 1 Managed Fields: <... snip ...> Resource Version: 394466 Self Link: /apis/keda.sh/v1alpha1/namespaces/default/scaledobjects/sample-app UID: 9394d57a-ae66-4e80-baf4-8d6bb7fd36f9 Spec: Advanced: Horizontal Pod Autoscaler Config: Behavior: Scale Down: Policies: Period Seconds: 15 Type: Percent Value: 100 Stabilization Window Seconds: 300 Restore To Original Replica Count: true Cooldown Period: 300 Max Replica Count: 10 Min Replica Count: 1 Polling Interval: 30 Scale Target Ref: API Version: apps/v1 Kind: Deployment Name: sample-app Triggers: Metadata: Metric Name: http_requests_total Query: sum ( rate ( http_requests_total { app = \"sample-app\" }[ 2m ])) Server Address: http://prom.castai:9090 Threshold: 1 Type: prometheus Status: Conditions: Message: ScaledObject doesn 't have correct scaleTargetRef specification Reason: ScaledObjectCheckFailed Status: False <--------- This means that this check didn' t pass Type: Ready Message: ScaledObject check failed Reason: UnkownState Status: Unknown Type: Active Events: <none> Inspect KEDA operator logs: kubectl logs -n keda $( kubectl get pods -n keda -o name | grep operator )","title":"Horizontal Pod Autoscaler"},{"location":"guides/pod-autoscaler/hpa/#horizontal-pod-autoscaler","text":"","title":"Horizontal Pod Autoscaler"},{"location":"guides/pod-autoscaler/hpa/#scaling-an-application","text":"You can scale an application in two ways: Vertically: by adding more resources (RAM/CPU/Disk IOPS) to the same instance, Horizontally: by adding more instances (replicas) of the same application. The problem with vertical scaling is that you'll either find out that the required hardware (RAM, CPU, Disk IOPS) in a single machine costs too much, or the cloud provider cannot provision a machine with enough resources. We use replica sets in Kubernetes to achieve horizontal scaling. The Horizontal Pod Autoscaler allows automating the process of maintaining the replica count proportionally to the application load.","title":"Scaling an application"},{"location":"guides/pod-autoscaler/hpa/#horizontal-scaling-strategy","text":"As mentioned above, the horizontal scaling strategy involves adding (or removing) the additional replicas of the same application. The problem here lies in the fact that most application load patterns can have spikes that are not predictable. This renders manual scaling nearly impossible. Luckily, we can automate this process!","title":"Horizontal scaling strategy"},{"location":"guides/pod-autoscaler/hpa/#the-hpa-keda","text":"Kubernetes comes equipped with the Horizontal Pod Autoscaler (HPA) functionality. It can scale up (add more replicas) or down (remove idling replicas) based on some metrics. The problem is that HPA, by default, comes without batteries: it doesn't have the metrics' source. But you're in luck: CAST AI bundles the batteries for you! We've got you covered by providing the KEDA addon.","title":"The HPA &amp; KEDA"},{"location":"guides/pod-autoscaler/hpa/#how-does-it-work","text":"KEDA consists of two components: operator -- watches k8s for ScaledObject resources and configures HPA accordingly metrics-apiserver -- a bridge between Kubernetes and various scaling sources (including Prometheus) These components do the heavy lifting of configuring Kubernetes HPA and setting up the custom metric sources. This enables us to autoscale almost any workload: Deployment , ReplicaSet , ReplicationController , or StatefulSet . KEDA does support autoscaling Jobs as well.","title":"How does it work"},{"location":"guides/pod-autoscaler/hpa/#enabling-keda","text":"In order to take advantage of the autoscaling functionality, you must enable KEDA addon in the Policies page: Navigate to an existing cluster (in case you don't have one already, go ahead and create one ) On the left navigation menu, select Policies : Enable the Horizontal pod autoscaler","title":"Enabling KEDA"},{"location":"guides/pod-autoscaler/hpa/#examples","text":"","title":"Examples"},{"location":"guides/pod-autoscaler/hpa/#autoscale-based-on-cpu-andor-memory-usage","text":"Let's create a Deployment and a Service that we will Autoscale : apiVersion : apps/v1 kind : Deployment metadata : name : sample-app labels : app : sample-app spec : # Note that we omit the replica count so # when we redeploy, we wouldn't override # replica count set by the autoscaler #replicas: 1 selector : matchLabels : app : sample-app template : metadata : labels : app : sample-app spec : containers : - image : luxas/autoscale-demo:v0.1.2 name : sample-app ports : - containerPort : 8080 --- apiVersion : v1 kind : Service metadata : name : sample-app labels : app : sample-app spec : ports : - port : 8080 name : http targetPort : 8080 protocol : TCP selector : app : sample-app Note : We don't specify the ReplicaCount ourselves Now let us set up a CPU-based Autoscaler apiVersion : keda.sh/v1alpha1 kind : ScaledObject metadata : name : sample-app spec : scaleTargetRef : name : sample-app minReplicaCount : 1 # Optional. Default: 0 maxReplicaCount : 10 # Optional. Default: 100 triggers : # Either of the triggers can be omitted. - type : cpu metadata : # Possible values: `Value`, `Utilization`, or `AverageValue`. # More info at: https://keda.sh/docs/2.0/scalers/cpu/#trigger-specification type : \"Value\" value : \"30\" - type : memory metadata : # Possible values: `Value`, `Utilization`, or `AverageValue`. # More info at: https://keda.sh/docs/2.0/scalers/memory/ type : \"Value\" value : \"512\" Now our Deployment autoscaling will be triggered either by CPU or Memory usage. We could use any other trigger, or remove either of those if we want (i.e. to autoscale only on the CPU basis, remove the Memory trigger, and vice-versa).","title":"Autoscale Based on CPU and/or Memory usage"},{"location":"guides/pod-autoscaler/hpa/#autoscale-based-on-the-prometheus-metric","text":"It's possible to autoscale based on the result of an arbitrary Prometheus query. What's more, CAST AI k8s clusters come with Prometheus deployed out of the box! Let's deploy the sample app again. But this time, let's instruct Prometheus to scrape metrics: apiVersion : apps/v1 kind : Deployment metadata : name : sample-app labels : app : sample-app spec : selector : matchLabels : app : sample-app template : metadata : labels : app : sample-app annotations : # These annotations the main difference! prometheus.io/path : \"/metrics\" prometheus.io/port : \"8080\" prometheus.io/scrape : \"true\" spec : containers : - image : luxas/autoscale-demo:v0.1.2 name : sample-app ports : - containerPort : 8080 --- apiVersion : v1 kind : Service metadata : name : sample-app labels : app : sample-app spec : ports : - port : 8080 name : http targetPort : 8080 protocol : TCP selector : app : sample-app Now let's deploy the Autoscaler! apiVersion : keda.sh/v1alpha1 kind : ScaledObject metadata : name : sample-app spec : scaleTargetRef : name : sample-app minReplicaCount : 1 # Optional. Default: 0 maxReplicaCount : 10 # Optional. Default: 100 triggers : - type : prometheus metadata : serverAddress : http://prom.castai:9090 metricName : http_requests_total_sample_app threshold : '1' # Note: query must return a vector/scalar single element response query : sum(rate(http_requests_total{app=\"sample-app\"}[2m])) Now let's generate some load and observe that the replica count is increased: # Deploy busybox image kubectl run -it --rm load-generator --image = busybox /bin/sh # Hit ENTER for command prompt # trigger infinite requests to the php-apache server while true ; do wget -q -O- http://sample-app:8080/metrics ; done # in order to cancel, hold CTRL+C # in order to quit, initiate CTRL+D sequence","title":"Autoscale based on the Prometheus metric"},{"location":"guides/pod-autoscaler/hpa/#troubleshooting","text":"Verify that KEDA is scheduled and running (the suffixes might be different): $ kubectl get pods -n keda NAME READY STATUS RESTARTS AGE keda-metrics-apiserver-59679c9f96-5lfr5 1 /1 Running 0 74m keda-operator-66744fc69d-7njdd 1 /1 Running 0 74m Describe ScaledObject for clues. In this case, scaledObjectRef points to nonexistent object: $ kubectl describe scaledobjects.keda.sh sample-app Name: sample-app Namespace: default Labels: scaledObjectName = sample-app Annotations: API Version: keda.sh/v1alpha1 Kind: ScaledObject Metadata: Creation Timestamp: 2020 -11-10T10:12:38Z Finalizers: finalizer.keda.sh Generation: 1 Managed Fields: <... snip ...> Resource Version: 394466 Self Link: /apis/keda.sh/v1alpha1/namespaces/default/scaledobjects/sample-app UID: 9394d57a-ae66-4e80-baf4-8d6bb7fd36f9 Spec: Advanced: Horizontal Pod Autoscaler Config: Behavior: Scale Down: Policies: Period Seconds: 15 Type: Percent Value: 100 Stabilization Window Seconds: 300 Restore To Original Replica Count: true Cooldown Period: 300 Max Replica Count: 10 Min Replica Count: 1 Polling Interval: 30 Scale Target Ref: API Version: apps/v1 Kind: Deployment Name: sample-app Triggers: Metadata: Metric Name: http_requests_total Query: sum ( rate ( http_requests_total { app = \"sample-app\" }[ 2m ])) Server Address: http://prom.castai:9090 Threshold: 1 Type: prometheus Status: Conditions: Message: ScaledObject doesn 't have correct scaleTargetRef specification Reason: ScaledObjectCheckFailed Status: False <--------- This means that this check didn' t pass Type: Ready Message: ScaledObject check failed Reason: UnkownState Status: Unknown Type: Active Events: <none> Inspect KEDA operator logs: kubectl logs -n keda $( kubectl get pods -n keda -o name | grep operator )","title":"Troubleshooting"}]}